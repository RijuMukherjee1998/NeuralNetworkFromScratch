{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ef7df4-a265-49c2-825e-25450ef60345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the case of ReLU activation function it is unbounded and not normalized. Not Normalized means that it can be anything like\n",
    "# [100, 10 ,-25] or [-8, 2000, 200] ... which doesn't give much context to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5cb6b7-5248-4cbc-a014-9b7d2e7f77be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So what a Softmax Activation Function does is that it takes some unbounded, not normalized input and outputs a normalized and bounded output\n",
    "# This distribution returned by the softmax activation function represents confidence scores for each class and will add up to 1. \n",
    "# The predicted class is associated with the output neuron that returned the largest confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df12c41-2f7a-4338-8fc5-ef29dec9e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still, we can also note the other confidence scores in our overarching algorithm/program that uses this network. \n",
    "# For example, if our network has a confidence distribution for two classes: [0.45, 0.55] , the prediction is the 2nd class, but the confidence in this\n",
    "# prediction isn’t very high. Maybe our program would not act in this case since it’s not very confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad0df5-eed5-444a-a9e6-2526daf56ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the function for the Softmax:\n",
    "# S_ij = (e^z_ij)/Summation(e^z_il)[l= 1 to L] ; e=constant = math.e = 2.71828182846(exponential)\n",
    "# z_ij indicates the output values from each of the neurons for that layer.\n",
    "# z_il indicates the output values from each of the neurons for that layer.\n",
    "# Both the numerator and the denominator of the Softmax function contain e raised to the power of z , where z , \\\n",
    "# given indices, means a singular output value — the index i means the current sample and the index j means the current output in this sample. \n",
    "# The numerator exponentiates the current output value and the denominator takes a sum of all of the exponentiated outputs for a given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb30489-0215-4abb-8a77-a1d8abae7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why was e used?\n",
    "# \"e\" is a monotonically increasing function which is almost 0 when e^x where x<0. So it will remove all negative nos and \n",
    "# dividing by the sum of all those exponential nos will result in values between 0 and 1. ~ Almost like probability for where\n",
    "# each probability lies between 0 and 1. Adding all of these values will eventually give 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "956c45e0-9678-40f2-8d82-aa7e479e5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def softmax_activation_func(layer_outputs):\n",
    "    E = math.e\n",
    "    numerator_exp_vals = []\n",
    "\n",
    "    #(e^z_ij) for each neuron output\n",
    "    for output in layer_outputs:\n",
    "        numerator_exp_vals.append(E ** output)\n",
    "    \n",
    "    print(numerator_exp_vals)\n",
    "\n",
    "    #Normalized Denomenator base for normalized values\n",
    "    # Summation(e^z_il)\n",
    "    norm_base = sum(numerator_exp_vals)\n",
    "\n",
    "    #normalized outputs after softmax activation\n",
    "    norm_outputs = []\n",
    "\n",
    "    for value in numerator_exp_vals:\n",
    "        norm_outputs.append(value/norm_base)\n",
    "    \n",
    "    print ( 'Normalized exponentiated values:' )\n",
    "    print (norm_outputs)\n",
    "    print ( 'Sum of normalized values:' , sum (norm_outputs)) # It will give a value very near to 1.0\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3baaa60b-bfb8-44a3-80c8-13cf70ebbb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n",
      "Normalized exponentiated values:\n",
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "Sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [ 4.8 , 1.21 , 2.385 ]\n",
    "softmax_activation_func(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a267b5c1-93ce-48b2-9287-04aad3a795c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Activation Function using numpy\n",
    "import numpy as np\n",
    "\n",
    "def softmax_activation_func_numpy(layer_outputs):\n",
    "    #(e^z_ij) for each neuron output\n",
    "    numerator_exp_vals = np.exp(layer_outputs)\n",
    "    \n",
    "    print(numerator_exp_vals)\n",
    "\n",
    "    #Normalized Denomenator base for normalized values\n",
    "    # Summation(e^z_il)\n",
    "    norm_base = np.sum(numerator_exp_vals)\n",
    "\n",
    "    #normalized outputs after softmax activation\n",
    "    norm_outputs = numerator_exp_vals/norm_base\n",
    "    \n",
    "    print ( 'Normalized exponentiated values:' )\n",
    "    print (norm_outputs)\n",
    "    print ( 'Sum of normalized values:' , sum (norm_outputs)) # It will give a value very near to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1764ef8-dd15-4869-983f-58fdeb2a2f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041752   3.35348465  10.85906266]\n",
      "Normalized exponentiated values:\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "Sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [ 4.8 , 1.21 , 2.385 ]\n",
    "softmax_activation_func_numpy(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fc5dc65-0654-4369-9106-e9c87824a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Activation Function using numpy with input batches\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax_activation_numpy_batch(layer_outputs_batch):\n",
    "    #(e^z_ij) for each neuron output\n",
    "    exp_vals_batch = np.exp(layer_outputs_batch)\n",
    "    \n",
    "    print(exp_vals_batch)\n",
    "\n",
    "    #normalized outputs after softmax activation\n",
    "    probabilities = exp_vals_batch / np.sum(exp_vals_batch, axis=1, keepdims=True)\n",
    "    \n",
    "    print ( 'Normalized exponentiated values:' )\n",
    "    print (probabilities)\n",
    "    print ( 'Sum of normalized values:' , np.sum(probabilities, axis=1, keepdims=False)) # It will give a value very near to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3f10529-2def-4019-9e1b-b6d5f66d2a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.21510418e+02 3.35348465e+00 1.08590627e+01]\n",
      " [7.33197354e+03 1.63654137e-01 1.22140276e+00]\n",
      " [4.09595540e+00 2.86051020e+00 1.02634095e+00]]\n",
      "Normalized exponentiated values:\n",
      "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
      " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
      " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n",
      "Sum of normalized values: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs_batch = np.array([[ 4.8 , 1.21 , 2.385 ],\n",
    "                                [ 8.9 , - 1.81 , 0.2 ],\n",
    "                                [ 1.41 , 1.051 , 0.026 ]])\n",
    "softmax_activation_numpy_batch(layer_outputs_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "883de4e2-62ff-427f-bdc4-f15152458e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Activation_Softmax:\n",
    "    #Forward pass\n",
    "    def forward(self, inputs):\n",
    "        #Get unnormalized input batch (inputs - max(input)). It is done so that the values if they are bigger like in the order of 1000 then e^1000\n",
    "        # calculation may fail as it can overflow. So keeping the inputs in a small -ve range will give a +ve output with values between 0 and 1\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        #Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1c1d1a0-e27b-49c0-ad88-31d74c31dfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
      " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
      " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
     ]
    }
   ],
   "source": [
    "activation_softmax1 = Activation_Softmax()\n",
    "layer_outputs_batch = np.array([[ 4.8 , 1.21 , 2.385 ],\n",
    "                                [ 8.9 , - 1.81 , 0.2 ],\n",
    "                                [ 1.41 , 1.051 , 0.026 ]])\n",
    "activation_softmax1.forward(layer_outputs_batch)\n",
    "print(activation_softmax1.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
