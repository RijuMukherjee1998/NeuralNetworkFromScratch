{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0312d476-de5f-4413-a2fb-402dec7f2a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the combined code i.e all the learnings are applied\n",
    "# togeather and used to form the whole neural n/w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5252fe1d-3279-4a9f-b33c-b97bca0d87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4531e4e4-9823-4434-863a-a3e728245604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        #Initialize weights and biases\n",
    "        #np.random.randn is a gaussian distribution which will provide values between -1 and 1 and 0.01 is multiplied to make the values smaller \n",
    "        # and close to each other thus fitting of data will be easier during training\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) #To avoid transposing every time (inputs, neurons) is used rather than (neurons, inputs) \n",
    "        self.biases = np.zeros((1, n_neurons)) #default bias initialization\n",
    "        pass\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #Calculate output values from inputs,weights and biases\n",
    "        print(\"Weights are = \",self.weights)\n",
    "        print(\"Biases are = \",self.biases)\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a082188-a36d-4892-b5fc-2c12e6659e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu Activation\n",
    "class Activation_ReLU:\n",
    "    def activate(self, inputs):\n",
    "        self.output = np.maximum(0,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c335cebe-1e5c-4295-a279-28d062da1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Activation\n",
    "class Activation_Softmax:\n",
    "    #Forward pass\n",
    "    def activate(self, inputs):\n",
    "        #Get unnormalized input batch (inputs - max(input)). It is done so that the values if they are bigger like in the order of 1000 then e^1000\n",
    "        # calculation may fail as it can overflow. So keeping the inputs in a small -ve range will give a +ve output with values between 0 and 1\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        #Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57781838-bd89-4893-88cd-f24c1dc6927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Loss Class uses Cross Entropy Loss\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        #return loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5950a1f-947d-4fb2-9c22-5700cbe1e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "\n",
    "    #Forward Pass\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # No of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # clip the data to avoid log(0) error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        # checking whether the class target/real values are sparse labels or one hot encoded\n",
    "        if(len(y_true.shape) == 1):\n",
    "            output = []\n",
    "            for targ_idx, distribution in zip(y_true, y_pred_clipped):\n",
    "                output.append(distribution[targ_idx])\n",
    "\n",
    "            loss_list = -np.log(output)\n",
    "            #Calculating the Average loss\n",
    "            avg_loss = np.mean(loss_list)\n",
    "            #print(\"Avg Loss = \", avg_loss)\n",
    "            \n",
    "        elif(len(y_true.shape) == 2):\n",
    "            outputs = []\n",
    "            sum = 0\n",
    "            for i in range(0,samples):\n",
    "                sum = 0\n",
    "                for k in range(0,len(y_true[0])):\n",
    "                    sum = sum + y_true[i][k]*np.log(y_pred_clipped[i][k])\n",
    "                outputs.append(sum)\n",
    "\n",
    "            avg_loss = -1/N * np.sum(outputs)\n",
    "            #print(\"Avg Loss = \",avg_loss)\n",
    "        \n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b288d76-26ad-4cc9-99eb-1c2db8ffd6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Create dataset\n",
    "    X, y = spiral_data(samples=100, classes=3)\n",
    "    \n",
    "    dense_layer1 = Layer_Dense(2, 3)\n",
    "\n",
    "    dense_layer1.forward(X)\n",
    "    \n",
    "    activation1 = Activation_ReLU() #Activation function \n",
    "\n",
    "    activation1.activate(dense_layer1.output) #Activates the neurons using ReLU and it's not normalized\n",
    "\n",
    "    dense_layer2 = Layer_Dense(3,3)\n",
    "\n",
    "    dense_layer2.forward(activation1.output)\n",
    "    \n",
    "    activation2 = Activation_Softmax()\n",
    "    activation2.activate(dense_layer2.output) #Passing through softmax activation at the output layer\n",
    "    \n",
    "    print(\"Dense Layer 2 Output After Softmax Activation::\")\n",
    "    print(activation2.output[:5]) # output from the activation function\n",
    "\n",
    "    loss_function = Loss_CategoricalCrossEntropy()\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "    print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "836a4668-2c1d-471e-a3f9-de5857c98dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are =  [[-0.00189899  0.01188112  0.00603885]\n",
      " [ 0.01809998 -0.00730415 -0.0048045 ]]\n",
      "Biases are =  [[0. 0. 0.]]\n",
      "Weights are =  [[ 0.02523316 -0.00779414 -0.00210855]\n",
      " [ 0.01729084  0.00037448  0.01480814]\n",
      " [ 0.00067967 -0.00054518 -0.00270281]]\n",
      "Biases are =  [[0. 0. 0.]]\n",
      "Dense Layer 2 Output After Softmax Activation::\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333454 0.33333256 0.3333329 ]\n",
      " [0.3333357  0.33333181 0.33333248]\n",
      " [0.33333616 0.33333146 0.33333239]\n",
      " [0.33333826 0.33333017 0.33333157]]\n",
      "loss: 1.0986117131151507\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d8e209-611b-4acf-930d-86b6fceb083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Calculation\n",
    "# Accuracy describes how often the largest confidence is the correct class in terms of\n",
    "# fraction.\n",
    "# To find the accuracy we will take the argmax value from the softmax outputs and then compare\n",
    "# these to the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ba1d5e-98ca-42ea-a8fa-6ce541e2fe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions =  [0 0 1]\n",
      "Truth =  [0 1 1]\n",
      "Accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1],\n",
    "                            [0.5, 0.1, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([0,1,1])\n",
    "\n",
    "# Calculate the values along the second axis(rows) and returns the index of that element\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "\n",
    "# If the targets are one-hot encoded -convert them\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "# True evaluates to 1 False = 0\n",
    "accuracy = np.mean(predictions==class_targets)\n",
    "\n",
    "print(\"Predictions = \",predictions)\n",
    "print(\"Truth = \",class_targets)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c2ed2-2585-4157-9683-dd98295be5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
