{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28893289-232a-4b32-8cab-aff4799ed783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Step Activition Function\n",
    "# y = {\n",
    "#       1  x > 0\n",
    "#       0  x<= 0 \n",
    "#     }\n",
    "# This activiton function was used historically but it is now rarely used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bfb31d-f053-467e-9b6c-8468888cad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Linear Activiton Function\n",
    "# This will appear as a tilted straight line when graphed\n",
    "# y = mx + c ; m = slope and if m=1 and c = 0 then y = x\n",
    "#\n",
    "# This activition function is applied to the last layer's output in the case of a regression model.\n",
    "# Regression Model - It is the type of model that output's a scalar value instead of a classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19dcc10-ad9e-4fb1-a6da-2327f9864430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sigmoid Activition Function\n",
    "# The biggest disadvantage of step function is that it is not very informative. It is either an on state or an off state. Due to this it is \n",
    "# missing the transition values that the optimizer need to understand how close or far was it from turning the neuron on or off.\n",
    "# Therefore we need a more granular function that will give a more informative feedback and one of the granular activation function is\n",
    "# Sigmoid Activation Function.\n",
    "#\n",
    "# y = (1 / (1 + e^-x))\n",
    "\n",
    "# The output of the sigmoid function always lies between 0 and 1. As x approaches positive infinity, y approaches 1. \n",
    "# As x approaches negative infinity, y approaches 0. The midpoint, where x is 0, results in y being exactly 0.5.\n",
    "\n",
    "# In mathematical terms, the range of values for y is [0, 1], inclusive. \n",
    "# However, in practice, due to the properties of the exponential function, the output values of the sigmoid function quickly \n",
    "# become very close to 0 or 1 as |x| increases, so most values you encounter will be very near 0 or 1.\n",
    "\n",
    "# Also the sigmoid function adds non-linearity to the function, which is very important.\n",
    "# The Sigmoid Function is historically used in the hidden layers but was eventually replaced by Rectified Linear Units (ReLU).\n",
    "\n",
    "# Note: Sigmoid Functions can be used as the Output Layer's activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d06c1-1428-4838-b33c-0154761c398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Rectified Linear Activation Function (ReLU)\n",
    "# y = {\n",
    "#       x  x>0\n",
    "#       0  x<=0\n",
    "#     }\n",
    "\n",
    "# The rectified linear activation function is simpler than the sigmoid. It’s quite literally y=x , clipped\n",
    "# at 0 from the negative side. If x is less than or equal to 0 , then y is 0 — otherwise, y is equal to x .\n",
    "\n",
    "# This is the most widely used activition function mainly due to it's speed and efficiency.\n",
    "# While the sigmoid function is not too complicated it is still much efficient to compute ReLU as in the hidden layers there are\n",
    "# tens af thousands of neurons and overall using sigmoid will consume more compute time as there is division,power operation in Sigmoid\n",
    "# function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7437e-b9e1-434c-a730-3c387acb3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why use Activation Function?\n",
    "\n",
    "# In most cases, for a neural network to fit a nonlinear function, we need it to contain two or more hidden layers, and\n",
    "# we need those hidden layers to use a nonlinear activation function.\n",
    "# The main attraction for neural networks has to do with their ability to solve nonlinear problems.\n",
    "# E.g. The number of factors that come into play while gueesing the price of a home are such as size, location, time of year attempting to sell, number of\n",
    "# rooms, yard, neighborhood, and so on, makes the pricing of a home a nonlinear equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875d1b4-31cb-44e8-9b07-2040ecc57092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why using Linear Activation in the hidden layers is bad?\n",
    "# Lets take an example where a neuron has a input of size 3 with 3 different weights\n",
    "# Activition_Function => [y = x]\n",
    "# input = [1.0, 2.0, -3.0]\n",
    "# weights = [1.0, 0.0, 0.0]\n",
    "# bias = 0.0\n",
    "# output = Activition_Function(input.weights + bias) = Activition_Function(1) = 1.0\n",
    "# Now as we continue to tweak with negative nos this time:\n",
    "# input = [1.0, 2.0, -3.0]\n",
    "# weights = [-1.0, 0.0, 0.0]\n",
    "# bias = 0.0\n",
    "# output = Activition_Function(input.weights + bias) = Activition_Function(-1) = -1.0\n",
    "# Now updating weights and biases:\n",
    "# input = [1.0, 2.0, -3.0]\n",
    "# weights = [0.0, 0.0, 1.0]\n",
    "# bias = 2.0\n",
    "# output = Activition_Function(input.weights + bias) = Activition_Function(-1) = -1.0\n",
    "#\n",
    "# No matter what we do with this neuron’s weights and biases, the output of this neuron will be\n",
    "# perfectly linear to y=x of the activation function. This linear nature will continue throughout the entire network:\n",
    "# however many layers we have, this network can only depict linear\n",
    "# relationships if we use linear activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62f74c-0389-48f6-b145-3bdc699bc151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU Activation in a Pair of Neurons (Plz check the graph in the nnfs book for further understanding)\n",
    "# With 1 neuron :\n",
    "# weight = 0, bias =0 , Activition Function = ReLU (y=0 for x<=0 and y=x for x>0)\n",
    "# For any input x with limit -1 to 1\n",
    "# output y = 0 graph is a straight line lying on the x-axis\n",
    "\n",
    "# With 1 neuron :\n",
    "# weight = 1, bias =0 , Activition Function = ReLU (y=0 for x<=0 and y=x for x>0)\n",
    "# For any input x with limit -1 to 1\n",
    "# output:: Graph is a line lying on the x-asis if x<=0 and y=x if x>0\n",
    "\n",
    "# With 1 neuron :\n",
    "# weight = 1, bias =0.50 , Activition Function = ReLU (y=0 for x<=0 and y=x for x>0)\n",
    "# For any input x with limit -1 to 1\n",
    "# output:: (if x<=-0.5) graph is a line lying on the x-asis  and y=x if x>-0.50 \n",
    "# [The activation is happeing earlier with +ve bias]\n",
    "\n",
    "# With 1 neuron :\n",
    "# weight = -1.0, bias =0.50 , Activition Function = ReLU (y=0 for x<=0 and y=x for x>0)\n",
    "# For any input x with limit -1 to 1\n",
    "# output:: (if x>=0.5), graph is a line lying on the x-asis and y=x if x<0.50 \n",
    "# [Now this is not activation rather it shows when the neuron deactivates]\n",
    "\n",
    "# With 2 neurons with 2 hidden layers :\n",
    "# weight1 = -1.0, bias1 =0.50 , weight2=1.0, bias2=0.0 and Activition Function = ReLU (y=0 for x<=0 and y=x for x>0)\n",
    "# For any input x with limit -1 to 1\n",
    "# output:: (if x>=0.5), graph is a line lying on the x-asis and y=x (if x<0.5)\n",
    "# [Now this is not activation rather it shows when the neuron deactivates] and it remains unchanged\n",
    "\n",
    "# With 2 neurons with 2 hidden layers :\n",
    "# weight1 = -1.0, bias1 =0.50 , weight2 = 1.0, bias2 = 1.0 and Activition Function = ReLU (y=0 for x<=0 and y=x for x>0)\n",
    "# For any input x with limit -1 to 1\n",
    "# output:: (if x>=0.50), graph is a line lying parallel to the x-asis with y = 1.0 constant \n",
    "# and (if x<0.50) then y=x.\n",
    "# [Now this is not activation rather it shows when the neuron deactivates]\n",
    "# Point to Note is that now the whole function is shifted to the y-axis by a value of 1.0\n",
    "\n",
    "# ***\n",
    "# With 2 neurons with 2 hidden layers :\n",
    "# weight1 = -1.0, bias1 =0.50 , weight2 = -1.0, bias2 = 1.0 and Activition Function = ReLU (y=0 for x<=0 and y=x for x>0)\n",
    "# For any input x with limit -1 to 1\n",
    "# output:: (if x<=0.0), graph is a line lying on the x-asis \n",
    "# and (if x>0 and x<0.5) then y=x.\n",
    "# otherwise with (x>=0.5) the graph is a line parallel to x axis where y = 1 \n",
    "# [Now this is an activation function not a decativation function]\n",
    "# Note this function has now a lower and upper bound between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ef80d2-0df7-4c9e-8086-1695f24a4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the neural n/w is fitting the sine wave check from nnfs book (Page - 81)\n",
    "# i.e. with an optimizer to tweak the weights and biases we will see with a x-value what\n",
    "# will be the y-value that will eventually try to replicate a sine wave where input x is\n",
    "# in between 0 to 1.\n",
    "# Note: It shows us how more neurons can enable more unique areas of effect and why we need two\n",
    "# or more hidden layers and why we need a nonlinear activation function.\n",
    "# Like in the example given a neural network of (1, 64, 64, 1) is much more true to the \n",
    "# correct result than the (1, 8, 8, 1) n/w."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
