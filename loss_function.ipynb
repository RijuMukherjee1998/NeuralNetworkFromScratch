{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53822620-3353-4a0a-b78c-254c29dbb405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Train a model we tweak the weights and biases to improve the models accuracy.\n",
    "# To do this we have to calculate how much error does a model have. \n",
    "# This algorithm that quantifies how wrong a model is reffered to as a loss function or\n",
    "# a cost function. Ideally we want this loss function to be 0.\n",
    "# The more confidence a model has the lesser is the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24677aa9-d35b-486d-b0e0-95e7a67184d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function used with neural netwoek that does the regression: squared error \n",
    "# (or mean squared error with neural networks)\n",
    "\n",
    "\n",
    "# Categorical cross-entropy is explicitly used to compare a \"ground-truth\" probability (y-targets)\n",
    "# to some prdicted distribution (y-hat or predictions), so it makes sense to use cross-entropy here.\n",
    "# It is also one of the most commonly used loss functions with a softmax activation on the output\n",
    "# layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a8008-8e71-4455-bfb8-588c47efb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The formula for calculating the categorical cross-entropy of y and y-hat is\n",
    "# L_i = - Summation(y_i,j * log(y-hat_i,j))[summation over j]\n",
    "\n",
    "# Where L_i denotes the sample loss value, i is the i-th sample in the set, j is the label/output\n",
    "# index, y denotes the target values, and y-hat denotes the predicted values.\n",
    "\n",
    "# Once we start coding the solution, we’ll simplify it further to -log(correct_class_confidence), the\n",
    "# formula for which is:\n",
    "# L_i = -log(y-hat_i,k) ; where k is an index of \"true\" probability\n",
    "\n",
    "# Where L_i denotes sample loss value, i is the i-th sample in a set, k is the index of the target label\n",
    "# (ground-true label), y denotes the target values and y-hat denotes the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194fa67d-74a0-4501-935c-3294e14dfeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "# Example code to find the cross-entropy loss given the Groud reality result\n",
    "import numpy as np\n",
    "\n",
    "softmax_output = [0.7, 0.1, 0.2] #Example output\n",
    "\n",
    "target_output = [1, 0, 0] #Groud truth\n",
    "\n",
    "loss = -(np.log(softmax_output[0])*target_output[0] + \n",
    "         np.log(softmax_output[1])*target_output[1] +\n",
    "         np.log(softmax_output[2])*target_output[2])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52fb2069-10dc-4c11-9621-21397dd1f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Categorical Cross-Entropy Loss accounts for that and outputs a larger loss the lower\n",
    "# the confidence is.\n",
    "# We’ve printed different log values for a few example confidences. When the confidence level\n",
    "# equals 1, meaning the model is 100% “sure” about its prediction, the loss value for this sample\n",
    "# equals 0. The loss value raises with the confidence level, approaching 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b40f7-0382-4f54-99f0-bcd96b678418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Loss Calculation for Sparse Labels is\n",
    "# L= -1/N(Summation i=1 to N(log(y_i,t_i)))\n",
    "# t_i is the target value and  for sample i and the y_i,t_i is the predicted value or confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04ea11fa-a730-4e08-b275-ab9034e99849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.e-01 1.e-01 2.e-01]\n",
      " [1.e-01 5.e-01 4.e-01]\n",
      " [1.e-07 9.e-01 8.e-02]]\n",
      "[0.7, 0.5, 0.9]\n",
      "[0.35667494 0.69314718 0.10536052]\n",
      "Avg Loss =  0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# probability of 3 samples softmax output batch\n",
    "softmax_outputs_raw = np.array([[0.7, 0.1, 0.2],\n",
    "                               [0.1, 0.5, 0.4],\n",
    "                               [0.0, 0.9, 0.08]])\n",
    "# Before directly using the Softmax Outputs we have to understand that it may contain 0 in certain\n",
    "# places so -log(0) is infinte which will throw an error so to solve this we will be clipping the\n",
    "# predicted range of values from 1e^-7 to (1 - 1e^-7)\n",
    "# So this will insignificantly impact the output but will get rid of the log(0) problem.\n",
    "softmax_outputs = np.array(np.clip(softmax_outputs_raw, 1e-7, 1 - 1e-7))\n",
    "print(softmax_outputs)\n",
    "\n",
    "# Here each row indicates probabilities of [dog, cat, human]\n",
    "# Lets say we have 3 classifications to do so we have received 3 subarrays of data from softmax. \n",
    "\n",
    "#class targets are the actual training data and it shows that out of 3 samples 1st one is dog\n",
    "# second is a cat and the 3rd sample is human.[0,1,2] (Sparese Labels)\n",
    "# But the current training set represents [0,1,1] a dog and 2 cats no humans in the training\n",
    "# sample yet.\n",
    "class_targets = [0,1,1] #dog, cat1, cat2 \n",
    "\n",
    "#It will store the confidences of the class target indices for each of the sample.\n",
    "output = []\n",
    "for targ_idx, distribution in zip(class_targets, softmax_outputs):\n",
    "    output.append(distribution[targ_idx])\n",
    "\n",
    "#it has the list of samples wrt the class targets.\n",
    "print(output)\n",
    "\n",
    "#After loss calculation\n",
    "loss_list = -np.log(output)\n",
    "\n",
    "print(loss_list)\n",
    "#Calculating the Average loss\n",
    "avg_loss = np.mean(loss_list)\n",
    "print(\"Avg Loss = \", avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9088df19-458c-4b7e-b503-0979f2520fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Calculation for One Hot Encoded Labels (i.e. each classification is done with only o and 1's)\n",
    "# Like dog = [1,0,0], cat = [0,1,0] and human = [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ac610f-1af2-4077-9b0f-8417ca8c3e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For One Hot Encoded Labels the Loss calculation formula is \n",
    "# L = -1/N(Summation i=1 to N (Summation k=1 to K(t_i,k*log(y_i,k))))\n",
    "# t_i,k = target value for sample i and class k and y_i,k is the predicted value or confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2218ca-3ceb-4143-9054-6b5b3d38a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.e-01 1.e-01 2.e-01]\n",
      " [1.e-01 5.e-01 4.e-01]\n",
      " [1.e-07 9.e-01 8.e-02]]\n",
      "Avg Loss =  0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_outputs_raw = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.0, 0.9, 0.08]])\n",
    "# Before directly using the Softmax Outputs we have to understand that it may contain 0 in certain\n",
    "# places so -log(0) is infinte which will throw an error so to solve this we will be clipping the\n",
    "# predicted range of values from 1e^-7 to (1 - 1e^-7)\n",
    "# So this will insignificantly impact the output but will get rid of the log(0) problem.\n",
    "\n",
    "softmax_outputs = np.array(np.clip(softmax_outputs_raw, 1e-7, 1 - 1e-7))\n",
    "print(softmax_outputs)\n",
    "\n",
    "# [1,0,0] means \n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "outputs = []\n",
    "sum = 0\n",
    "N = len(softmax_outputs)\n",
    "for i in range(0,len(softmax_outputs)):\n",
    "    sum = 0\n",
    "    for k in range(0,len(class_targets[0])):\n",
    "        sum = sum + class_targets[i][k]*np.log(softmax_outputs[i][k])\n",
    "    outputs.append(sum)\n",
    "\n",
    "avg_loss = -1/N * np.sum(outputs)\n",
    "print(\"Avg Loss = \",avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe61e03-2238-43b9-beeb-661cb6f67fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Cross-Entropy Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e024cf6-d9a7-4480-b950-b5964da4fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss =  0.38506088005216804\n",
      "Loss =  0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class Loss_CategoricalCrossEntropy():\n",
    "\n",
    "    #Forward Pass\n",
    "\n",
    "    def Forward(self, y_pred, y_true):\n",
    "        # No of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # clip the data to avoid log(0) error\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        # checking whether the class target/real values are sparse labels or one hot encoded\n",
    "        if(len(y_true.shape) == 1):\n",
    "            output = []\n",
    "            for targ_idx, distribution in zip(y_true, y_pred_clipped):\n",
    "                output.append(distribution[targ_idx])\n",
    "\n",
    "            loss_list = -np.log(output)\n",
    "            #Calculating the Average loss\n",
    "            avg_loss = np.mean(loss_list)\n",
    "            print(\"Avg Loss = \", avg_loss)\n",
    "            \n",
    "        elif(len(y_true.shape) == 2):\n",
    "            outputs = []\n",
    "            sum = 0\n",
    "            for i in range(0,samples):\n",
    "                sum = 0\n",
    "                for k in range(0,len(y_true[0])):\n",
    "                    sum = sum + y_true[i][k]*np.log(y_pred_clipped[i][k])\n",
    "                outputs.append(sum)\n",
    "\n",
    "            avg_loss = -1/N * np.sum(outputs)\n",
    "            print(\"Avg Loss = \",avg_loss)\n",
    "        \n",
    "        return avg_loss\n",
    "\n",
    "def test():\n",
    "    softmax_outputs_raw = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.0, 0.9, 0.08]])\n",
    "    class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "    loss_function = Loss_CategoricalCrossEntropy()\n",
    "    loss = loss_function.Forward(softmax_outputs_raw,class_targets)\n",
    "    print(\"Loss = \",loss)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169aa704-7b11-47f9-8f39-c8378cf669e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
