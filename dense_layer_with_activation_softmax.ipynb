{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11feaaa6-6bb1-40b2-a269-81d12103ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dense layer will have each neurons which has a output and has a activation function and has softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36754cc-ea8f-4abe-a962-787e47c0b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        #Initialize weights and biases\n",
    "        #np.random.randn is a gaussian distribution which will provide values between -1 and 1 and 0.01 is multiplied to make the values smaller \n",
    "        # and close to each other thus fitting of data will be easier during training\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) #To avoid transposing every time (inputs, neurons) is used rather than (neurons, inputs) \n",
    "        self.biases = np.zeros((1, n_neurons)) #default bias initialization\n",
    "        pass\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #Calculate output values from inputs,weights and biases\n",
    "        print(\"Weights are = \",self.weights)\n",
    "        print(\"Biases are = \",self.biases)\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        pass\n",
    "        \n",
    "class Activation_ReLU:\n",
    "    def activate(self, inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "\n",
    "class Activation_Softmax:\n",
    "    #Forward pass\n",
    "    def activate(self, inputs):\n",
    "        #Get unnormalized input batch (inputs - max(input)). It is done so that the values if they are bigger like in the order of 1000 then e^1000\n",
    "        # calculation may fail as it can overflow. So keeping the inputs in a small -ve range will give a +ve output with values between 0 and 1\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        #Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2776d509-feb2-4f55-a5e8-50fffac2bd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are =  [[-0.01306527  0.01658131 -0.00118164]\n",
      " [-0.00680178  0.00666383 -0.0046072 ]]\n",
      "Biases are =  [[0. 0. 0.]]\n",
      "Dense Layer 1 output:: \n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
      " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
      " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
      " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n",
      "Dense Layer 1 Output After Activation::\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n",
      "Weights are =  [[-0.01334258 -0.01346717  0.00693773]\n",
      " [-0.00159573 -0.00133702  0.01077744]\n",
      " [-0.01126826 -0.00730678 -0.0038488 ]]\n",
      "Biases are =  [[0. 0. 0.]]\n",
      "Dense Layer 2 output:: \n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.8183968e-07 -1.5235776e-07  1.2281279e-06]\n",
      " [-5.0631292e-07 -4.2422371e-07  3.4195891e-06]\n",
      " [-8.4041352e-07 -7.0415609e-07  5.6760728e-06]\n",
      " [-1.1393766e-06 -9.5464793e-07  7.6952419e-06]]\n",
      "Dense Layer 2 Output After Softmax Activation::\n",
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n"
     ]
    }
   ],
   "source": [
    "#Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "#creating a dense layer with 2 input feature(size of each input = 2) and 3 output values(3 neurons)\n",
    "neurons_layer1 = 3\n",
    "dense_layer1 = Layer_Dense(len(X[0]), neurons_layer1)\n",
    "\n",
    "dense_layer1.forward(X[:5]) #Takes only 5 rows of data or input-batch of 5 data rows \n",
    "\n",
    "print(\"Dense Layer 1 output:: \")\n",
    "print(dense_layer1.output) #For every input batch what is the value from each neuron(3 neurons (columns))\n",
    "\n",
    "activation1 = Activation_ReLU() #Activation function \n",
    "\n",
    "activation1.activate(dense_layer1.output) #Activates the neurons using ReLU and it's not normalized\n",
    "\n",
    "print(\"Dense Layer 1 Output After Activation::\")\n",
    "print(activation1.output) # output from the activation function\n",
    "\n",
    "input_batch_layer2 = activation1.output\n",
    "neurons_layer2 = 3\n",
    "dense_layer2 = Layer_Dense(len(input_batch_layer2[0]),neurons_layer2)\n",
    "\n",
    "dense_layer2.forward(input_batch_layer2)\n",
    "\n",
    "print(\"Dense Layer 2 output:: \")\n",
    "print(dense_layer2.output) #For every input batch what is the value from each neuron(3 neurons (columns))\n",
    "\n",
    "smax_activation = Activation_Softmax()\n",
    "smax_activation.activate(dense_layer2.output) #Passing through softmax activation at the output layer\n",
    "\n",
    "print(\"Dense Layer 2 Output After Softmax Activation::\")\n",
    "print(smax_activation.output) # output from the activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e2664-b8cf-4391-ba70-b00f5eda9dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
